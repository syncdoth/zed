# Zed Edit Prediction Proxy Configuration

# OpenAI-compatible API endpoint (vLLM, Ollama, LM Studio, etc.)
openai_base_url: "http://localhost:1234/v1" # LM Studio default
# openai_base_url: "http://localhost:11434/v1"  # Ollama default
# openai_base_url: "http://localhost:8000/v1"  # vLLM default

# Model name to use - should match your local model
# model_name: "zed-industries/zeta"
# model_name: "codellama:7b-code"  # Example for Ollama
# model_name: "deepseek-coder:6.7b"  # Example for Ollama
model_name: "zed-industries-zeta"

# Model parameters
max_tokens: 300
temperature: 0.1
timeout: 10.0

# System prompt for the model
system_prompt: |
  You are Zeta, an AI assistant that predicts code edits based on user input and context.

  Your task is to analyze the provided code context and predict what edits the user likely wants to make. Focus on:
  1. Understanding the current code structure and context
  2. Predicting logical next steps or completions
  3. Maintaining code style and conventions
  4. Being concise and accurate

  Respond only with the predicted code changes, not explanations or commentary.
  If the input suggests a completion, provide the completion.
  If the input suggests a refactoring or edit, provide the modified code.

  Examples of good responses:
  - For completing a function: return the function body
  - For fixing a bug: return the corrected code
  - For adding a feature: return the new code to add

  Always match the existing code style, indentation, and naming conventions.
